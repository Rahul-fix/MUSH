{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cfba354",
   "metadata": {},
   "source": [
    "# COCO-style Semantic Segmentation with Mask2Former\n",
    "This notebook demonstrates how to set up a semantic segmentation pipeline using a COCO-style dataset and the Mask2Former model. It covers dataset preparation, label remapping, color palette setup, and data loading for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e5aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/MUSH/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Consolidated Imports\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.special import softmax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    "    Mask2FormerImageProcessor,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a76c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Random Seeds for Reproducibility\n",
    "seed = 78\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151301d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO Dataset Class Definition\n",
    "class COCODataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset class for COCO-format JSON annotations and images.\n",
    "    Each item returns:\n",
    "      - 'image': a PIL Image (converted to RGB)\n",
    "      - 'semantic_map': a 2D uint8 tensor where each pixel's value is the category ID\n",
    "      - 'image_id': the original COCO image ID\n",
    "      - 'width', 'height': dimensions of the image\n",
    "    \"\"\"\n",
    "    def __init__(self, coco_file: str, root_dir: str, split: str = None, transform=None):\n",
    "        with open(coco_file, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        self.split_image_ids = { # dictionary mapping split names to image IDs\n",
    "            'train': list(range(283, 314)) + list(range(314, 345)) + list(range(408, 471)),\n",
    "            'valid': list(range(345, 377)) + list(range(533, 564)),\n",
    "            'test':  list(range(377, 408)) + list(range(471, 533))\n",
    "        }\n",
    "        all_images = self.coco_data['images']\n",
    "        if split in self.split_image_ids:\n",
    "            valid_ids = set(self.split_image_ids[split])\n",
    "            self.images = [img for img in all_images if img['id'] in valid_ids]\n",
    "        else:\n",
    "            self.images = all_images\n",
    "        self.annotations = self.coco_data['annotations']\n",
    "        self.categories = {\n",
    "            cat['id']: {\n",
    "                'name': cat['name'],\n",
    "                'color': cat.get('color', \"#000000\"),\n",
    "                'supercategory': cat['supercategory']\n",
    "            }\n",
    "            for cat in self.coco_data['categories']\n",
    "        }\n",
    "        print(\"Category IDs and their names:\")\n",
    "        for cat_id, cat_info in self.categories.items():\n",
    "            print(f\"  ID {cat_id}: {cat_info['name']}\")\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_id_to_annotations = {}\n",
    "        for anno in self.annotations:\n",
    "            image_id = anno['image_id']\n",
    "            if image_id not in self.image_id_to_annotations:\n",
    "                self.image_id_to_annotations[image_id] = []\n",
    "            self.image_id_to_annotations[image_id].append(anno)\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_info = self.images[idx]\n",
    "        image_id = image_info['id']\n",
    "        width, height = image_info['width'], image_info['height']\n",
    "        relative_path = image_info['path'].lstrip('/datasets/')\n",
    "        image_path = os.path.join(self.root_dir, relative_path)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        annotations = self.image_id_to_annotations.get(image_id, [])\n",
    "        segmentations = [anno.get('segmentation', []) for anno in annotations]\n",
    "        category_ids = [anno['category_id'] for anno in annotations]\n",
    "        semantic_map = np.zeros((height, width), dtype=np.uint8)\n",
    "        for seg, cat_id in zip(segmentations, category_ids):\n",
    "            for poly in seg:\n",
    "                coords = np.array(poly).reshape(-1, 2)\n",
    "                rr, cc = skimage.draw.polygon(coords[:, 1], coords[:, 0], semantic_map.shape)\n",
    "                semantic_map[rr, cc] = cat_id\n",
    "        semantic_map_tensor = torch.tensor(semantic_map, dtype=torch.uint8)\n",
    "        return {\n",
    "            'image': image,\n",
    "            'semantic_map': semantic_map_tensor,\n",
    "            'image_id': image_id,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e326c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Collate Function for DataLoader\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    collated = {}\n",
    "    for key in batch[0]:\n",
    "        if key == 'image':\n",
    "            collated['images'] = torch.stack([item['image'] for item in batch])\n",
    "        elif key == 'semantic_map':\n",
    "            collated['semantic_map'] = torch.stack([item['semantic_map'] for item in batch])\n",
    "        else:\n",
    "            collated[key] = [item[key] for item in batch]\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d36313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remapped ID-to-label: {0: 'bg', 1: 'pepper_kp', 2: 'pepper_red', 3: 'pepper_yellow', 4: 'pepper_green', 5: 'pepper_mixed', 6: 'pepper_mixed_red', 7: 'pepper_mixed_yellow'}\n",
      "Color palette (RGB):\n",
      " [[  0   0   0]\n",
      " [  0   0 255]\n",
      " [199  33  28]\n",
      " [255 247   0]\n",
      " [  0 255   0]\n",
      " [225   0 255]\n",
      " [255 102   0]\n",
      " [209 196  21]]\n"
     ]
    }
   ],
   "source": [
    "# ID-to-Label and Color Palette Setup\n",
    "id2label = {\n",
    "    0: \"bg\",\n",
    "    11: \"pepper_kp\",\n",
    "    12: \"pepper_red\",\n",
    "    13: \"pepper_yellow\",\n",
    "    14: \"pepper_green\",\n",
    "    15: \"pepper_mixed\",\n",
    "    17: \"pepper_mixed_red\",\n",
    "    18: \"pepper_mixed_yellow\"\n",
    "}\n",
    "label2id = {old_id: new_id for new_id, old_id in enumerate(sorted(id2label.keys()))}\n",
    "id2label_remapped = {new_id: id2label[old_id] for old_id, new_id in label2id.items()}\n",
    "print(\"Remapped ID-to-label:\", id2label_remapped)\n",
    "id2color = {\n",
    "    0: \"#000000\",\n",
    "    1: \"#0000ff\",\n",
    "    2: \"#c7211c\",\n",
    "    3: \"#fff700\",\n",
    "    4: \"#00ff00\",\n",
    "    5: \"#e100ff\",\n",
    "    6: \"#ff6600\",\n",
    "    7: \"#d1c415\",\n",
    "}\n",
    "palette = []\n",
    "for class_id in range(len(id2label_remapped)):\n",
    "    hex_color = id2color.get(class_id, \"#000000\")\n",
    "    rgb = tuple(int(hex_color.lstrip(\"#\")[i : i + 2], 16) for i in (0, 2, 4))\n",
    "    palette.append(rgb)\n",
    "palette = np.array(palette, dtype=np.uint8)\n",
    "print(\"Color palette (RGB):\\n\", palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f35758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Function to Remap Mask Labels\n",
    "def remap_labels(mask: np.ndarray, label2id_map: dict) -> torch.Tensor:\n",
    "    if not isinstance(mask, torch.Tensor):\n",
    "        mask = torch.tensor(mask, dtype=torch.int64)\n",
    "    remapped_mask = torch.zeros_like(mask)\n",
    "    for old_id, new_id in label2id_map.items():\n",
    "        remapped_mask[mask == old_id] = new_id\n",
    "    return remapped_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d36b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper Dataset: ImageSegmentationDataset\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A wrapper around a base dataset (e.g. COCODataset) to:\n",
    "      - Remap original class IDs in the mask to contiguous IDs\n",
    "      - Apply image transforms (normalization, augmentation) to the input image\n",
    "      - Optionally apply target transforms to the segmentation mask\n",
    "    Returns a tuple: (image_tensor, remapped_mask_tensor, original_image_numpy, original_mask_numpy)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset: Dataset, transform=None, target_transform=None):\n",
    "        self.dataset = base_dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.dataset[idx]\n",
    "        orig_pil_image = sample['image']\n",
    "        orig_mask_np = np.array(sample['semantic_map'])\n",
    "        remapped_mask = remap_labels(orig_mask_np, label2id)\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(orig_pil_image)\n",
    "        else:\n",
    "            image_tensor = torch.tensor(np.array(orig_pil_image), dtype=torch.float32).permute(2, 0, 1)\n",
    "        if self.target_transform:\n",
    "            mask_transformed = self.target_transform(Image.fromarray(remapped_mask.numpy()))\n",
    "            mask_tensor = torch.tensor(np.array(mask_transformed), dtype=torch.int64)\n",
    "        else:\n",
    "            mask_tensor = remapped_mask\n",
    "        return image_tensor, mask_tensor, np.array(orig_pil_image), orig_mask_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01502a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Image and Target Transforms\n",
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255.0\n",
    "ADE_STD  = np.array([58.395,  57.120,  57.375]) / 255.0\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    # e.g. transforms.RandomHorizontalFlip(p=1.0)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b26a539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category IDs and their names:\n",
      "  ID 11: pepper_kp\n",
      "  ID 12: red\n",
      "  ID 13: yellow\n",
      "  ID 14: green\n",
      "  ID 15: mixed\n",
      "  ID 17: mixed_red\n",
      "  ID 18: mixed_yellow\n",
      "Category IDs and their names:\n",
      "  ID 11: pepper_kp\n",
      "  ID 12: red\n",
      "  ID 13: yellow\n",
      "  ID 14: green\n",
      "  ID 15: mixed\n",
      "  ID 17: mixed_red\n",
      "  ID 18: mixed_yellow\n",
      "Category IDs and their names:\n",
      "  ID 11: pepper_kp\n",
      "  ID 12: red\n",
      "  ID 13: yellow\n",
      "  ID 14: green\n",
      "  ID 15: mixed\n",
      "  ID 17: mixed_red\n",
      "  ID 18: mixed_yellow\n",
      "Category IDs and their names:\n",
      "  ID 11: pepper_kp\n",
      "  ID 12: red\n",
      "  ID 13: yellow\n",
      "  ID 14: green\n",
      "  ID 15: mixed\n",
      "  ID 17: mixed_red\n",
      "  ID 18: mixed_yellow\n",
      "Category IDs and their names:\n",
      "  ID 11: pepper_kp\n",
      "  ID 12: red\n",
      "  ID 13: yellow\n",
      "  ID 14: green\n",
      "  ID 15: mixed\n",
      "  ID 17: mixed_red\n",
      "  ID 18: mixed_yellow\n",
      "Sample shapes (train_dataset[0]):\n",
      "  image tensor shape = torch.Size([3, 1280, 720])\n",
      "  remapped mask shape = torch.Size([1280, 720])\n",
      "  original image shape = (1280, 720, 3)\n",
      "  original mask shape = (1280, 720)\n",
      "Sample shapes (train_dataset[0]):\n",
      "  image tensor shape = torch.Size([3, 1280, 720])\n",
      "  remapped mask shape = torch.Size([1280, 720])\n",
      "  original image shape = (1280, 720, 3)\n",
      "  original mask shape = (1280, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/vcm_ssmn09q_34hqj0s_cfjm0000gn/T/ipykernel_20635/1427035855.py:21: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  orig_mask_np = np.array(sample['semantic_map'])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Base and Wrapped Datasets\n",
    "coco_file_path   = os.path.expanduser(\"~/Downloads/Thesis/CKA_sweet_pepper_2020_summer/CKA_sweet_pepper_2020_summer.json\")\n",
    "dataset_root_dir = os.path.expanduser(\"~/Downloads/Thesis\")\n",
    "\n",
    "base_train_ds = COCODataset(coco_file=coco_file_path, root_dir=dataset_root_dir, split='train', transform=None)\n",
    "base_val_ds   = COCODataset(coco_file=coco_file_path, root_dir=dataset_root_dir, split='valid', transform=None)\n",
    "base_test_ds  = COCODataset(coco_file=coco_file_path, root_dir=dataset_root_dir, split='test', transform=None)\n",
    "\n",
    "train_dataset = ImageSegmentationDataset(base_train_ds, transform=train_transform, target_transform=None)\n",
    "valid_dataset = ImageSegmentationDataset(base_val_ds,   transform=train_transform, target_transform=None)\n",
    "test_dataset  = ImageSegmentationDataset(base_test_ds,  transform=test_transform,  target_transform=None)\n",
    "\n",
    "# Quick sanity check: print shapes for first sample\n",
    "image_tensor, mask_tensor, orig_img_np, orig_mask_np = train_dataset[0]\n",
    "print(\"Sample shapes (train_dataset[0]):\")\n",
    "print(\"  image tensor shape =\", image_tensor.shape)\n",
    "print(\"  remapped mask shape =\", mask_tensor.shape)\n",
    "print(\"  original image shape =\", orig_img_np.shape)\n",
    "print(\"  original mask shape =\", orig_mask_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ec986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 62\n",
      "Number of valid batches: 32\n"
     ]
    }
   ],
   "source": [
    "# Prepare Mask2Former Processor and DataLoaders\n",
    "preprocessor = Mask2FormerImageProcessor(\n",
    "    ignore_index=255,\n",
    "    reduce_labels=False,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=False,\n",
    "    num_labels=len(id2label_remapped)\n",
    ")\n",
    "\n",
    "def segmentation_collate_fn(batch):\n",
    "    images, masks, orig_images, orig_masks = zip(*batch)\n",
    "    processed = preprocessor(\n",
    "        list(images),\n",
    "        segmentation_maps=list(masks),\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Attach original images and masks for later use (e.g. metric computation)\n",
    "    processed[\"original_images\"] = orig_images\n",
    "    processed[\"original_segmentation_maps\"] = orig_masks\n",
    "    return processed\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=segmentation_collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=segmentation_collate_fn)\n",
    "test_dataloader  = DataLoader(test_dataset,  batch_size=2, shuffle=False, collate_fn=segmentation_collate_fn)\n",
    "\n",
    "print(f\"Number of train batches: {len(train_dataloader)}\")\n",
    "print(f\"Number of valid batches: {len(valid_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ff9ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([9, 256]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]/var/folders/_5/vcm_ssmn09q_34hqj0s_cfjm0000gn/T/ipykernel_20635/1427035855.py:21: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  orig_mask_np = np.array(sample['semantic_map'])\n",
      "/var/folders/_5/vcm_ssmn09q_34hqj0s_cfjm0000gn/T/ipykernel_20635/1427035855.py:21: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  orig_mask_np = np.array(sample['semantic_map'])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training Loop for Semantic Segmentation\n",
    "# Use train_dataloader, valid_dataloader, test_dataloader, preprocessor, id2label_remapped from previous cells\n",
    "# Make sure 'model' is defined in a previous cell or define it here if not already present\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the Mask2Former model for universal segmentation\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "    \"facebook/mask2former-swin-large-ade-semantic\",\n",
    "    num_labels=len(id2label_remapped),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=2e-4)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=5e-6)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "running_loss = 0.0\n",
    "num_samples = 0\n",
    "\n",
    "def get_class_labels(mask_labels):\n",
    "    return [torch.zeros_like(lbl, dtype=torch.int64) for lbl in mask_labels]\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        class_labels = get_class_labels(batch[\"mask_labels\"])\n",
    "        outputs = model(\n",
    "            pixel_values=batch[\"pixel_values\"].to(device),\n",
    "            mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "            class_labels=[labels.to(device) for labels in class_labels],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        batch_size = batch[\"pixel_values\"].size(0)\n",
    "        running_loss += loss.item()\n",
    "        num_samples += batch_size\n",
    "        if idx % 100 == 0:\n",
    "            print(\"Loss:\", running_loss/num_samples)\n",
    "        optimizer.step()\n",
    "        # scheduler.step(epoch)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for idx, batch in enumerate(tqdm(valid_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            class_labels = get_class_labels(batch[\"mask_labels\"])\n",
    "            outputs = model(\n",
    "                pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "                class_labels=[labels.to(device) for labels in class_labels],\n",
    "            )\n",
    "            valid_loss = outputs.loss\n",
    "        val_loss += valid_loss.item()\n",
    "    print(\"Mean IoU:\", metric.compute(num_labels = len(id2label_remapped ), ignore_index=0)['mean_iou'])\n",
    "    avg_val_loss = val_loss / len(valid_dataloader)\n",
    "    print(\"Validation Loss:\", avg_val_loss)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        model_save_path = f\"~/best_model_epoch_{best_epoch}.pt\"\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved at epoch {best_epoch} with validation loss: {best_val_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MUSH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
